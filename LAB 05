# =============================================================================
# BDAE Lab 5: Data Cleaning, Preprocessing, and Logistic Regression Modeling
# (Consolidated Code from 2219_BDAE_Lab_5.ipynb)
# =============================================================================

# --- 1. Setup, Imports, and Data Loading (Pandas) ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# NOTE: The 'files.upload()' call is kept as in the original notebook, 
# although it requires interactive execution in Colab.
uploaded = files.upload()
print("Saving Leads.csv to Leads.csv")

# Load the dataset
df = pd.read_csv('Leads.csv')
df.head()

# --- 2. Initial Missing Value Assessment ---
missing_count = df.isnull().sum()
missing_percentage = (missing_count / len(df)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_count,
    'Missing %': missing_percentage.round(2)
}).sort_values(by='Missing %', ascending=False)

print("\nMissing Values Summary:")
print(missing_summary)

# --- 3. Column and Row Dropping for Missing Data ---
# Identify columns to drop (missing > 40%)
cols_to_drop = missing_percentage[missing_percentage > 40].index
# Identify columns for row dropping (missing < 20%)
cols_to_drop_less_20 = missing_percentage[missing_percentage < 20].index

print("\nColumns to drop (missing > 40%):")
print(list(cols_to_drop))

# Drop columns with > 40% missing values
df_cleaned = df.drop(columns=cols_to_drop)

# Drop rows with missing values in columns where missing % < 20%
df_cleaned = df_cleaned.dropna(subset=cols_to_drop_less_20)

# Check remaining missing values summary
remaining_missing = df_cleaned.isnull().sum()
remaining_percentage = (remaining_missing / len(df_cleaned)) * 100

missing_summary_after = pd.DataFrame({
    'Missing Values': remaining_missing,
    'Missing %': remaining_percentage.round(2)
}).sort_values(by='Missing %', ascending=False)

print("\nMissing Values Summary After Cleaning:")
print(missing_summary_after)

print(f"\nFinal dataset shape: {df_cleaned.shape}")

# --- 4. Missing Value Imputation (Median/Mode) ---
# Separate numeric and categorical columns based on the 'df_cleaned' state
numeric_cols = df_cleaned.select_dtypes(include=['number']).columns
categorical_cols = df_cleaned.select_dtypes(include=['object']).columns

print("\nMissing values BEFORE imputation:")
print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])

# Fill missing values for numeric columns with median
for col in numeric_cols:
    median_value = df_cleaned[col].median()
    df_cleaned[col].fillna(median_value, inplace=True)

# Fill missing values for categorical columns with mode
for col in categorical_cols:
    mode_value = df_cleaned[col].mode()[0] if not df_cleaned[col].mode().empty else 'Unknown'
    df_cleaned[col].fillna(mode_value, inplace=True)

# Verify missing values are handled
print("\nMissing values AFTER imputation:")
print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])

# Save the cleaned dataset (needed for the next modeling step)
df_cleaned.to_csv("Leads_Cleaned.csv", index=False)
print("\nCleaned dataset saved as 'Leads_Cleaned.csv'")


# --- 5. Logistic Regression Model Training (BEFORE Outlier Treatment) ---

# Load the cleaned dataset (the one just saved)
df = pd.read_csv("Leads_Cleaned.csv")

# Define target and features
target = 'Converted'
X = df.drop(columns=[target])
y = df[target]

# Encode categorical variables (LabelEncoder for binary, get_dummies for multi-category)
le = LabelEncoder()
for col in X.select_dtypes(include=['object']).columns:
    if X[col].nunique() == 2:
        X[col] = le.fit_transform(X[col])
    else:
        X = pd.get_dummies(X, columns=[col], drop_first=True)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Build and train the Logistic Regression model
model = LogisticRegression(max_iter=1000, solver='liblinear')
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model performance (Model 1: Before Outliers)
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("\n=== Logistic Regression Model Performance (BEFORE Outlier Treatment) ===")
print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(cm)
print("\nClassification Report:")
print(report)


# --- 6. Model Visualization (Model 1: BEFORE Outlier Treatment) ---

# Plot Confusion Matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot Model Accuracy
plt.figure(figsize=(4,4))
sns.barplot(x=["Logistic Regression"], y=[accuracy], palette="viridis")
plt.ylim(0,1)
plt.title(f"Model Accuracy: {accuracy*100:.2f}%")
plt.ylabel("Accuracy")
plt.show()


# --- 7. Outlier Treatment (IQR Capping) and Model Re-Training ---

# Copy the original cleaned dataframe before capping
df_out = df_cleaned.copy()

# Detect numeric columns
numeric_cols = df_out.select_dtypes(include=['number']).columns

# Function to cap outliers using IQR
def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # Replace extreme values
    df[col] = np.where(df[col] < lower, lower, df[col])
    df[col] = np.where(df[col] > upper, upper, df[col])

# Apply IQR capping to all numeric columns
for col in numeric_cols:
    cap_outliers(df_out, col)

# Prepare data again for Logistic Regression (Model 2: AFTER Outliers)
target = "Converted"
X = df_out.drop(columns=[target])
y = df_out[target]

# Encode categorical values (same as earlier)
le = LabelEncoder()
for col in X.select_dtypes(include=['object']).columns:
    if X[col].nunique() == 2:
        X[col] = le.fit_transform(X[col])
    else:
        X = pd.get_dummies(X, columns=[col], drop_first=True)

# Split data
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

# Train Logistic Regression again
model2 = LogisticRegression(max_iter=1000, solver='liblinear')
model2.fit(X_train2, y_train2)

# Predictions
y_pred2 = model2.predict(X_test2)

# NEW accuracy after outlier treatment
accuracy_after = accuracy_score(y_test2, y_pred2)

print(f"\nAccuracy BEFORE outlier treatment: {accuracy}")
print(f"Accuracy AFTER outlier treatment: {accuracy_after}")


# --- 8. Model Visualization (Model 2: AFTER Outlier Treatment) ---

# Confusion Matrix Plot (Model 2: AFTER Outliers)
cm2 = confusion_matrix(y_test2, y_pred2)
plt.figure(figsize=(5,4))
sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix AFTER Outlier Treatment")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Accuracy Improvement Bar Plot
plt.figure(figsize=(5,4))
sns.barplot(
    x=["Before Outliers", "After Outliers"],
    y=[accuracy, accuracy_after],
    palette=["red", "green"]
)
plt.ylim(0,1)
plt.ylabel("Accuracy")
plt.title("Logistic Regression Accuracy Improvement")
plt.show()
